Hi, I'm Vachan. I like to build and train Deep Neural Networks from scratch.

# Projects:
[**NeuroForge**](https://github.com/VachanVY/NeuroForge):
* [Neural Networks](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#neural-networks)
  * [Logistic Regression](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#logistic-regression)
  * [MLP](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#logistic-regression)
    * [Forward Propagation (Explained on Pen and Paper)](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#logistic-regression)
    * [Back Propagation (Equations Derived on Pen and Paper)](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#logistic-regression)
    * [Gradient Descent](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#logistic-regression)
    * [Train Loop](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#logistic-regression)
    * [Results](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#results)
  * [Batch-Normalization and Layer-Normalization: **Why When Where & How?**](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#results)
    * [Batch-Normalization](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#batch-normalization)
    * [Layer-Normalization](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#layer-normalization)
    * [Comparision](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#comparision)
  * [Dropout: **Why When Where & How?**](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#dropout-paper-deep-learning-book)
    * [Comparision before and after scaling the model](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#comparision-1)
  * [Adam and AdamW](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#adam-and-adamw-adam-with-weight-decay-optimizers)
    * [Adam](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#adam-and-adamw-adam-with-weight-decay-optimizers)
    * [AdamW](https://github.com/VachanVY/NeuroForge?tab=readme-ov-file#adam-and-adamw-adam-with-weight-decay-optimizers)


[**gpt.jax**](https://github.com/VachanVY/gpt.jax): GPT written in jax, trained on the tinystories dataset

[**Vision-Transformers**](https://github.com/VachanVY/Vision-Transformers): Vision Transformers in jax, trained on MNIST dataset

[**Diffusion-Transformers**](https://github.com/VachanVY/Diffusion-Transformers): On going project... Not Public for now...

## [**Mugen**]()
* Going to make a website for music generation completely from scratch using *Pytorch*. On going project...
* ### Models for this Project
  * Non Autoregressive Transformer
  * Autoregressive Transformer
  * Diffusion Transformer

